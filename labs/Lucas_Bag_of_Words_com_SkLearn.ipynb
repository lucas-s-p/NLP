{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIq6MoiJ62f9"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfD96eaj8fzX"
      },
      "source": [
        "CountVectorizer tokeniza, cria o vocabulário e conta a frequência de ocorrência das palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S5M159J6yDF"
      },
      "source": [
        "docs=[\"the house had a tiny little mouse\",\n",
        "      \"the cat saw the mouse\",\n",
        "      \"the mouse ran away from the house\",\n",
        "      \"the cat finally ate the mouse\",\n",
        "      \"the end of the mouse story\"\n",
        "     ]\n",
        "#instancia CountVectorizer()\n",
        "cv=CountVectorizer()\n",
        "#uma matriz esparsa é retornada com a contagem das palavras para cada documento.\n",
        "word_count_vector=cv.fit_transform(docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McCot1bU7YSu",
        "outputId": "17d59fac-f914-415a-f6be-9e9c57d5adae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_count_vector.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EGuGrOWFB4u",
        "outputId": "81761f12-3740-4a4d-a6d4-641e6d26e0a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(word_count_vector.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1]\n",
            " [0 0 1 0 0 0 0 0 0 1 0 0 1 0 2 0]\n",
            " [0 1 0 0 0 1 0 1 0 1 0 1 0 0 2 0]\n",
            " [1 0 1 0 1 0 0 0 0 1 0 0 0 0 2 0]\n",
            " [0 0 0 1 0 0 0 0 0 1 1 0 0 1 2 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01sfb0JHEbP0",
        "outputId": "4ff20780-1c78-4be0-a94a-6e50cd03aa4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cv.get_feature_names_out()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ate', 'away', 'cat', 'end', 'finally', 'from', 'had', 'house',\n",
              "       'little', 'mouse', 'of', 'ran', 'saw', 'story', 'the', 'tiny'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au0SeDFy-UuP"
      },
      "source": [
        "É possível escoher o tokenizador, stopwords, n-gramas e muitos outros parâmetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUr7Dbn8E7Vo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c77678a3-63a8-41a4-8e97-dbda68ee58a1"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer('\\w+')\n",
        "\n",
        "cv=CountVectorizer()\n",
        "\n",
        "cv.set_params(tokenizer=tokenizer.tokenize)\n",
        "\n",
        "# remove stop words\n",
        "cv.set_params(stop_words='english')\n",
        "\n",
        "# considera 1-gramas e 2-gramas\n",
        "cv.set_params(ngram_range=(1, 2))\n",
        "\n",
        "# ignore terms that appear in more than 50% of the documents\n",
        "#vect.set_params(max_df=0.5)\n",
        "\n",
        "# only keep terms that appear in at least 2 documents\n",
        "#vect.set_params(min_df=2)\n",
        "\n",
        "word_count_vector=cv.fit_transform(docs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnFFRDRaGcrJ",
        "outputId": "62ab5b4d-08a6-4ffd-c4a8-6d8e638365f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_count_vector.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 25)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc8srUyyA0kc",
        "outputId": "4d79757f-736b-4a62-b548-3fe7a0b6279d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cv.get_feature_names_out()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ate', 'ate mouse', 'away', 'away house', 'cat', 'cat finally',\n",
              "       'cat saw', 'end', 'end mouse', 'finally', 'finally ate', 'house',\n",
              "       'house tiny', 'little', 'little mouse', 'mouse', 'mouse ran',\n",
              "       'mouse story', 'ran', 'ran away', 'saw', 'saw mouse', 'story',\n",
              "       'tiny', 'tiny little'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Nu3DtqGmFH",
        "outputId": "78a22f2d-463e-4d87-9d7f-67b3449f66f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_count_vector.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 1],\n",
              "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
              "        0, 0, 0],\n",
              "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
              "        0, 0, 0],\n",
              "       [1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "        1, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6gptKPh-71H"
      },
      "source": [
        "TfidfVectorizer permite construir vetores TF*IDF para cada documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a3vsfszGm_u",
        "outputId": "bb64a3cb-9708-4f94-ab7a-ceba70660e40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# instancia um objeto da classe TfidfVectorizer\n",
        "tfidf_vectorizer=TfidfVectorizer()\n",
        "\n",
        "# passa a lista de documentos para vetorização tf-idf\n",
        "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)\n",
        "\n",
        "tfidf_vectorizer_vectors.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.49356209, 0.39820278, 0.49356209, 0.23518498,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.23518498,\n",
              "        0.49356209],\n",
              "       [0.        , 0.        , 0.48334378, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.28547062,\n",
              "        0.        , 0.        , 0.59909216, 0.        , 0.57094124,\n",
              "        0.        ],\n",
              "       [0.        , 0.45709287, 0.        , 0.        , 0.        ,\n",
              "        0.45709287, 0.        , 0.36877965, 0.        , 0.2178072 ,\n",
              "        0.        , 0.45709287, 0.        , 0.        , 0.43561441,\n",
              "        0.        ],\n",
              "       [0.51392301, 0.        , 0.41462985, 0.        , 0.51392301,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.24488707,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.48977413,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.49175319, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.23432303,\n",
              "        0.49175319, 0.        , 0.        , 0.49175319, 0.46864606,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlgS3vfWTSp8"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}